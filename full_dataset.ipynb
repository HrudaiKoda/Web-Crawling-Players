{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUl-pR0TZaN-"
   },
   "outputs": [],
   "source": [
    "#Run any one cell to select a certain url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_vsk5BDtuAO"
   },
   "outputs": [],
   "source": [
    "#Alphabetical parse url\n",
    "url = \"https://www.espncricinfo.com/ci/content/player/country.html?country=6;alpha=A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNUM_ahiZOiJ"
   },
   "outputs": [],
   "source": [
    "#Country based parse url\n",
    "url = \"https://www.espncricinfo.com/ci/content/player/index.html?country=6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4UqagRqtuAS"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bf\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfN8Hh0YtuAS"
   },
   "outputs": [],
   "source": [
    "req = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01O7E2ZBtuAT"
   },
   "outputs": [],
   "source": [
    "soup = bf(req.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAMybnmjtuAT"
   },
   "outputs": [],
   "source": [
    "search_tag_container = soup.find_all('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7va7aMB6tuAU"
   },
   "outputs": [],
   "source": [
    "#Fetch total urls of players belonging to a country\n",
    "u = list()\n",
    "for job_elem in search_tag_container:\n",
    "    u.append(job_elem.find('a'))\n",
    "    #result.append(job_elem.text)\n",
    "l = set(u)\n",
    "l = list(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMcBcnhbjwX4"
   },
   "outputs": [],
   "source": [
    "def evaluate(html):\n",
    "  select_divs = html.find_all('div',class_=\"card overflow-hidden mb-3\")\n",
    "  exist = 0\n",
    "  location = 0\n",
    "  for i in range(len(select_divs)):\n",
    "    check = \"\"\n",
    "    try:\n",
    "      check = select_divs[i].find('p',class_=\"benton-bold pl-3 pt-4 pb-3 m-0 player-card-header\").text\n",
    "    except:\n",
    "      k = 1\n",
    "    if(check == \"Career Averages\"):\n",
    "      exist = 1\n",
    "      location = i\n",
    "      break\n",
    "  return exist,select_divs[location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling images\n",
    "def get_player_image(player_url):\n",
    "    # For fixing installation errors on linux: `sudo apt install firefox-geckodriver`\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(player_url)\n",
    "    time.sleep(2)\n",
    "    images = driver.find_elements_by_tag_name('img')\n",
    "    images = [image for image in images if \"player-card__face\" in image.get_attribute(\"class\")]\n",
    "    driver.close()\n",
    "    return [\"image\"], [images[0].get_attribute(\"src\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trophy info\n",
    "def get_major_trophies_information(major_trophies_soup):\n",
    "    major_trophies_results = {}\n",
    "    all_h5_tags = major_trophies_soup.find_all('h5', class_=\"border-bottom-gray-300 m-0 pl-3 pb-2 table-header\")\n",
    "    if all_h5_tags is None:\n",
    "        return None\n",
    "    required_h5_tag = all_h5_tags[0]\n",
    "    for h5_tag in all_h5_tags:\n",
    "        if str(h5_tag.text).strip() == \"In Major Trophies\":\n",
    "            required_h5_tag = h5_tag\n",
    "            break\n",
    "    major_trophies_table_head = required_h5_tag.findNext('thead')\n",
    "    major_trophies_table_headings = major_trophies_table_head.find_all('th')\n",
    "    major_trophies_table_headings_count = 0\n",
    "    major_trophies_headings_list = []\n",
    "    for table_heading in major_trophies_table_headings:\n",
    "        major_trophies_headings_list.append(table_heading.text)\n",
    "    major_trophies_table_headings_count = len(major_trophies_headings_list)\n",
    "    # print('Major Trophies table Head count', major_trophies_table_headings_count)\n",
    "    current_title = \"HOME\"\n",
    "    major_trophies_table_body = major_trophies_table_head.findNext('tbody')\n",
    "    major_trophies_table_data = major_trophies_table_body.find_all('td')\n",
    "    t = 0\n",
    "    for table_data in major_trophies_table_data:\n",
    "        attribute_number = t % major_trophies_table_headings_count\n",
    "        if(attribute_number != 0):\n",
    "            major_trophies_results[current_title][major_trophies_headings_list[attribute_number]] = table_data.text\n",
    "        else:\n",
    "            current_title = table_data.text\n",
    "            major_trophies_results[current_title] = {}\n",
    "        t += 1\n",
    "    print(major_trophies_results)\n",
    "    return [\"Major trophies\"], [major_trophies_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis - Home, Away stats and function call to trophy info\n",
    "def get_statistical_analysis_and_trophy_info(req):\n",
    "    analysis_labels = []\n",
    "    analysis_results = []\n",
    "    analysis_url = req.url + \"/bowling-batting-stats\"\n",
    "    #analysis_url = \"https://www.espncricinfo.com/player/virat-kohli-253802/bowling-batting-stats\"\n",
    "    \n",
    "    # selecting dropdown option\n",
    "    driver = webdriver.Firefox()\n",
    "    driver.get(analysis_url)\n",
    "    # loading page\n",
    "    time.sleep(2)\n",
    "    dropdowns = driver.find_elements_by_class_name('dropdown-container')\n",
    "    for dropdown in dropdowns:\n",
    "        is_format_dropdown = str(dropdown.find_elements_by_tag_name('button')[0].text).strip() in [\"Test\", \"ODI\", \"T20I\", \"Test+ODI+T20I\"]\n",
    "        is_role_dropdown = str(dropdown.find_elements_by_tag_name('button')[0].text).strip() in [\"Batting\", \"Bowling\", \"Fielding\", \"Allround\"]\n",
    "        if (not is_format_dropdown) and (not is_role_dropdown):\n",
    "            continue\n",
    "        required_span_text = \"Test+ODI+T20I\"\n",
    "        if is_role_dropdown:\n",
    "            required_span_text = \"Allround\"\n",
    "        dropdown.find_elements_by_tag_name('button')[0].click()\n",
    "        all_list_items = dropdown.find_elements_by_tag_name('li')\n",
    "        for list_item in all_list_items:\n",
    "            span_text = list_item.find_elements_by_tag_name('span')[0].text\n",
    "            if span_text == required_span_text:\n",
    "                list_item.click()\n",
    "                break\n",
    "    # waiting for changes to load\n",
    "    time.sleep(2)\n",
    "    analysis_page = driver.page_source\n",
    "    driver.close()\n",
    "    analysis_soup = bf(analysis_page, \"html.parser\")\n",
    "    \n",
    "    all_h5_tags = analysis_soup.find_all('h5', class_=\"border-bottom-gray-300 m-0 pl-3 pb-2 table-header\")\n",
    "    if all_h5_tags is None:\n",
    "        return None\n",
    "    required_h5_tag = all_h5_tags[0]\n",
    "    for h5_tag in all_h5_tags:\n",
    "        if str(h5_tag.text).strip() == \"Home vs Away\":\n",
    "            required_h5_tag = h5_tag\n",
    "            break\n",
    "    analysis_table_head = required_h5_tag.findNext('thead')\n",
    "    analysis_table_headings = analysis_table_head.find_all('th')\n",
    "    analysis_table_headings_count = 0\n",
    "    analysis_table_headings_list = []\n",
    "    for table_heading in analysis_table_headings:\n",
    "        analysis_table_headings_list.append(table_heading.text)\n",
    "    analysis_table_headings_count = len(analysis_table_headings_list)\n",
    "    # print('Analysis table Head count', analysis_table_headings_count)\n",
    "    current_title = \"HOME\"\n",
    "    analysis_table_body = analysis_table_head.findNext('tbody')\n",
    "    analysis_table_data = analysis_table_body.find_all('td')\n",
    "    t = 0\n",
    "    for table_data in analysis_table_data:\n",
    "        attribute_number = t % analysis_table_headings_count\n",
    "        if(attribute_number != 0):\n",
    "            analysis_results.append(table_data.text)\n",
    "            analysis_labels.append(current_title + \"_\" + analysis_table_headings_list[attribute_number])\n",
    "        else:\n",
    "            current_title = table_data.text.upper()\n",
    "        t += 1\n",
    "    # for i in range(len(analysis_labels)):\n",
    "    #     print(f'{analysis_labels[i]} : {analysis_results[i]}')\n",
    "    trophy_label, trophy_result = get_major_trophies_information(analysis_soup)\n",
    "    return analysis_labels + trophy_label, analysis_results + trophy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relations(Relations):\n",
    "    relations_container = Relations.find_next('div')\n",
    "    individual_relations = relations_container.find_all('span')\n",
    "    total_relations = list()\n",
    "    for i in range(len(individual_relations)):\n",
    "        name = individual_relations[i].find('h5').text\n",
    "        bond = individual_relations[i].find('p').text\n",
    "        total_relations.append([name,bond])\n",
    "    return total_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A_yiQzdtuAU"
   },
   "outputs": [],
   "source": [
    "#Get relevant information related to a player using each player's url\n",
    "def load(req,final_list_players,ind,u,debut_url):\n",
    "    lab = list()\n",
    "    result = list()\n",
    "    soup = bf(req.text,\"html.parser\")\n",
    "    print(soup.title)\n",
    "    search_tag_container = soup.find_all('h5',class_='player-card-description gray-900')\n",
    "    count = 0\n",
    "    for job_elem in search_tag_container:\n",
    "        result.append(job_elem.text)\n",
    "        count+=1\n",
    "    search_tag_container = soup.find_all('p',class_=\"text-uppercase gray-700 mb-0 pb-0-5 player-card-heading\")\n",
    "    for job_elem in search_tag_container:\n",
    "        lab.append(job_elem.text)\n",
    "    ##################################\n",
    "    Relations = soup.find('p',class_=\"text-uppercase gray-700 mb-0 player-card-heading text-uppercase\")\n",
    "    #relation_players = soup.find('p',class_=\"player-card-description player-description-link gray-900 m-0 player-link\")\n",
    "    if(Relations != None):\n",
    "        if(Relations.text.lower()=='relations' or Relations.text.lower()=='relation'):\n",
    "            lab.append(\"Relations\")\n",
    "            all_relations = find_relations(Relations)\n",
    "            result.append([all_relations])\n",
    "    ######################################\n",
    "    rec = extra(u)\n",
    "    lab.append(\"Records\")\n",
    "    result.append([rec])\n",
    "    print(result)\n",
    "    ##########################################\n",
    "    matches = debut(debut_url)\n",
    "    result.append([matches])\n",
    "    lab.append(\"Matches\")\n",
    "    ##########################################\n",
    "    lab.append(\"Teams\")\n",
    "    team_data = teams_data(soup)\n",
    "    result.append([team_data])\n",
    "    ###########################################\n",
    "    analysis_and_trophy_labels, analysis_and_trophy_results = get_statistical_analysis_and_trophy_info(req)\n",
    "    lab = lab + analysis_and_trophy_labels\n",
    "    result = result + analysis_and_trophy_results\n",
    "    ###########################################\n",
    "    player_image_label, player_image_result = get_player_image(req.url)\n",
    "    lab = lab + player_image_label\n",
    "    result = result + player_image_result\n",
    "    ###########################################\n",
    "    career_span_tag = soup.find('div', class_=\"intl_career-desktop\")\n",
    "    career_span_tag_text = career_span_tag.findNext('p').text\n",
    "    career_span_info = \"\"\n",
    "    start_index = 0\n",
    "    for i in range(len(career_span_tag_text)):\n",
    "        if career_span_tag_text[i] == '1' or career_span_tag_text[i] == '2':\n",
    "            start_index = i\n",
    "            break\n",
    "    career_span_tag_text = career_span_tag_text[start_index:]\n",
    "    result.append(career_span_tag_text)\n",
    "    lab.append(\"career_span\")\n",
    "    ###########################################\n",
    "    search_tag_container = soup.find('div',class_=\"player-card__details\")\n",
    "    l = list()\n",
    "    for job_elem in search_tag_container:\n",
    "        l.append(job_elem.text)\n",
    "        count+=1\n",
    "    p = l[1].split(\"|\")\n",
    "    l.pop()\n",
    "    l.pop()\n",
    "    l = l+p\n",
    "    result = result + l\n",
    "    l_label = [\"Player Name\" , \"Nationality \", \"player-type\"]\n",
    "    lab = lab + l_label\n",
    "    ##########################\n",
    "    valid,soup = evaluate(soup)\n",
    "    if(valid == 1):\n",
    "      search_tag_container = soup.findAll('thead')\n",
    "      if(len(search_tag_container)!=0):\n",
    "        tag_content = search_tag_container[0].find_all('th')\n",
    "        head = 0\n",
    "        table_labels = list()\n",
    "        table_labels_1 =list()\n",
    "        for job_elem in tag_content:\n",
    "            table_labels.append(job_elem.text)\n",
    "            head+=1\n",
    "        print('Head count',head)\n",
    "        \n",
    "        search_tag_container = soup.findAll('tbody')\n",
    "        tag_content = search_tag_container[0].find_all('td')\n",
    "        track_head_label = 0\n",
    "        for job_elem in tag_content:\n",
    "            if(track_head_label%head !=0):\n",
    "                result.append(job_elem.text)\n",
    "                count+=1\n",
    "            else:\n",
    "                table_labels_1.append(job_elem.text)\n",
    "            \n",
    "            track_head_label+=1\n",
    "        total_label1 = list()\n",
    "        for i in range(1,len(table_labels)):\n",
    "            for j in range(len(table_labels_1)):\n",
    "                total_label1.append('Batting' + table_labels[i] +\" \"+table_labels_1[j])\n",
    "        lab = lab + total_label1\n",
    "      ####################################\n",
    "      search_tag_container = soup.findAll('thead')\n",
    "      if(len(search_tag_container)>1):\n",
    "        tag_content = search_tag_container[1].find_all('th')\n",
    "        head = 0\n",
    "        table_labels = list()\n",
    "        table_labels_1 =list()\n",
    "        for job_elem in tag_content:\n",
    "            table_labels.append(job_elem.text)\n",
    "            head+=1\n",
    "        print('Head count',head)\n",
    "        \n",
    "        search_tag_container = soup.findAll('tbody')\n",
    "        tag_content = search_tag_container[1].find_all('td')\n",
    "        track_head_label = 0\n",
    "        for job_elem in tag_content:\n",
    "            if(track_head_label % head !=0):\n",
    "                result.append(job_elem.text)\n",
    "                count+=1\n",
    "            else:\n",
    "                table_labels_1.append(job_elem.text)\n",
    "            \n",
    "            track_head_label+=1\n",
    "        total_label1 = list()\n",
    "        for i in range(1,len(table_labels)):\n",
    "            for j in range(len(table_labels_1)):\n",
    "                total_label1.append('Bowling' + table_labels[i] +\" \"+table_labels_1[j])\n",
    "        lab = lab + total_label1\n",
    "    print('label--',len(lab))\n",
    "    print('result--',len(result))\n",
    "    \n",
    "    lo = dict(zip(lab,result))\n",
    "    lp = pd.DataFrame(lo,index=[ind])\n",
    "    final_list_players.append(lp)\n",
    "    print('count',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNzOA_ZttuAV"
   },
   "outputs": [],
   "source": [
    "#Get individual player's \"Records\" information.\n",
    "import re\n",
    "def extra(l):\n",
    "    re = requests.get(l)\n",
    "    time.sleep(2)\n",
    "    so = bf(re.text,\"html.parser\")\n",
    "    res1 = so.find_all('div',class_='col-14 d-flex flex-row')\n",
    "    res = so.find_all('div',class_='row bg-gray-100 class-record-row-p d-flex align-items-center')\n",
    "    #print(res1)\n",
    "    url_list = list()\n",
    "    for i in range(len(res1)):\n",
    "\n",
    "        pre = res1[i].find_all('h1')\n",
    "        loi = res1[i].find_all('p',class_='record-class-title m-0')\n",
    "        #print(loi)    \n",
    "        for j in range(len(pre)):\n",
    "            #print(pre[j].text)\n",
    "            #print(loi[j].text)\n",
    "            url_list.append(str(pre[j].text)  +str(\" \") + str(loi[j].text))\n",
    "    return url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFMqUqH6ON_2"
   },
   "outputs": [],
   "source": [
    "def teams_data(so):\n",
    "  teams = []\n",
    "  loi = so.find_all('h5',class_='m-0 ml-2 link-border-bottom player-description-link')\n",
    "  for i in range(len(loi)):\n",
    "    teams.append(loi[i].text)\n",
    "  return teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ho2v5lYXUjZ"
   },
   "outputs": [],
   "source": [
    "def debut(link):\n",
    "  player_match = list()\n",
    "  url = link\n",
    "  request = requests.get(url)\n",
    "  soup = bf(request.text,\"html.parser\")\n",
    "  search_tag_container = soup.find_all('div',{'id':'debut-last-matches'})\n",
    "  cp_jo = search_tag_container\n",
    "  if(len(search_tag_container) != 0):\n",
    "    total = cp_jo[0].find_all('div',class_=\"\")\n",
    "    for i in range(len(total)):\n",
    "      p =total[i]\n",
    "      a= p.find('h5',{'class':\"benton-bold player-matches-subtitle d-flex justify-content-between align-items-center player_matches-dropdown m-0\"})\n",
    "      k = total[i].find_all('span',class_=\"match-event-row-debut-last white-space-nowrap\")\n",
    "      search_tag_container = total[i].find_all('span',{\"class\": \"player-match-link\"})\n",
    "      new = total[i].find_all('div',{\"class\":\"col-12 player-match-event-cell white-space-nowrap\"})\n",
    "      #print(a)\n",
    "      #print(k[0])\n",
    "      #print(new)\n",
    "      try:\n",
    "        l =a.span.decompose()\n",
    "      except:\n",
    "        player_match.append(a.text)\n",
    "        #print(a.text)\n",
    "      else:\n",
    "        player_match.append(a.text)\n",
    "        #print(a.text)\n",
    "      for j in range(len(k)):\n",
    "        try:\n",
    "          temp = search_tag_container[j].text\n",
    "        except:\n",
    "          temp = new[j].text\n",
    "        append = k[j].text + str(\" \") + temp\n",
    "        player_match.append(append)\n",
    "        #print(k[j].text,end=\" \")\n",
    "        #print(search_tag_container[j].text)\n",
    "  return player_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9c0G1FctuAW",
    "outputId": "4642ca32-b810-4de7-a03e-5897bd506d89"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "final_list_players = list()\n",
    "for i in range(3):\n",
    "    u = \"https://www.espncricinfo.com\"\n",
    "    rl = l[i][\"href\"]\n",
    "    link = u+rl\n",
    "    print(link)\n",
    "    req = requests.get(link)\n",
    "    time.sleep(2)\n",
    "    print('player-',i,end=\" \")\n",
    "    ind = i-1\n",
    "    record_url = req.url+str(\"/tests-odi-t20-records\")\n",
    "    debut_url = req.url+str(\"/matches\")\n",
    "    #debut(debut_url)\n",
    "    load(req,final_list_players,ind,record_url,debut_url)\n",
    "    #print(record_url)\n",
    "    #jk = extra(record_url)\n",
    "    #print(jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "MDnjpeuGXJHe"
   },
   "outputs": [],
   "source": [
    "a = final_list_players[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "_tvPb2lHtuAa"
   },
   "outputs": [],
   "source": [
    "df = a.T\n",
    "for i in range(1,len(final_list_players)):\n",
    "    lo = final_list_players[i].T\n",
    "    df = pd.concat([df,lo],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nRJDjbatuAa",
    "outputId": "52de277c-62cf-40f3-d90b-b89e3f401b6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZ3REl0HXJHk"
   },
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of full_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (indic)",
   "language": "python",
   "name": "indic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
